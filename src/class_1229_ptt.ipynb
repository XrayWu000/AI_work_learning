{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbfdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "import bs4 as bs\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/M.1765596118.A.4A6.html\"\n",
    "resp = req.urlopen(url)\n",
    "content = resp.read()\n",
    "html = bs.BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca869196",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_text = \"-\"\n",
    "board_text = \"-\"\n",
    "title_text = \"-\"\n",
    "date_text = \"-\"\n",
    "push = []\n",
    "\n",
    "metas = html.find_all(\"span\", {\"class\": \"article-meta-value\"})\n",
    "if len(metas) == 4:\n",
    "    uid, board, title, date = metas\n",
    "    uid_text = uid.get_text().strip()\n",
    "    board_text = board.get_text().strip()\n",
    "    title_text = title.get_text().strip()\n",
    "    date_text = date.get_text().strip()\n",
    "\n",
    "data = {\n",
    "    \"uid\": uid_text,\n",
    "    \"board\": board_text,\n",
    "    \"title\": title_text,\n",
    "    \"datetime\": date_text,\n",
    "    \"pushes\": push}\n",
    "\n",
    "push_meta = html.find_all(\"div\", {\"class\": \"push\"})\n",
    "for p in push_meta:\n",
    "    p_dic = {}\n",
    "    p_tag, p_uid, p_text, p_ipdatetime = p.find_all(\"span\")\n",
    "    tag_text = p_tag.get_text().strip()\n",
    "    uid_text = p_uid.get_text().strip()\n",
    "    text_text = p_text.get_text().strip().replace(\": \", \"\", 1)\n",
    "    ipdatetime_text = p_ipdatetime.get_text().strip()\n",
    "    dic = {\"推 \": 1, \"噓 \": -1, \"→ \": 0}\n",
    "    type_value = dic.get(tag_text, 0)\n",
    "\n",
    "    p_dic = {\n",
    "        \"tag\": type_value,\n",
    "        \"uid\": uid_text,\n",
    "        \"text\": text_text,\n",
    "        \"ipdatetime\": ipdatetime_text\n",
    "    }\n",
    "    push.append(p_dic)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a98309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n退役前效力俱樂部：日本排球聯賽V2 GSS東京サンビームズ\\n\\n位置：自由人\\n\\n身高：156cm\\n\\n體重：52kg\\n\\n生日：2005-01-26\\n\\n指高：203cm\\n\\n最高到達點：256cm\\n\\n退役年份：2025-06-02\\n\\n\\nhttps://meee.com.tw/KL5bDOJ.jpg\\n\\nhttps://meee.com.tw/Pp2KGLY.jpg\\n\\nhttps://meee.com.tw/ENV8NFG.jpg\\n\\nhttps://meee.com.tw/H5jgPgI.jpg\\n\\nhttps://meee.com.tw/ITnYlbn.jpg\\n\\nhttps://meee.com.tw/ZYneBHR.jpg\\n\\nhttps://meee.com.tw/vRP7iE4.jpg\\n\\n--\\n\\n沒錯，18歲加入，今年退役\\n\\n自由人確實會比較矮一些，小島滿菜美也才158，但也是有身高高的\\n\\n你是說未知子?  XD\\n\\n\\n其實好看的不只木村紗織，不過木村紗織現在看起來有點嬸味了\\n\\n跟木村同期的狩野舞子也是滿漂亮但太玻璃，今年是嫁給WEST的桐山照史\\n\\n最後一張左邊的菊野樹奈也滿不錯的，今年也退役轉戰抖音\\n\\n現役球員還不錯的有野中瑠衣、中島咲愛\\n\\n佐藤淑乃不到表特但現在是日本女排主力，打球滿殺的\\n\\n歹勢，這個圖床我是第一次用晚點修，不然之前用imgur比較好\\n\\n但imgur現在會擋台灣ip\\n\\n可能放在表特沒太多推，但他算那種英氣型的，扣球滿煞氣的\\n\\n\\n她手臂肌肉線條滿明顯，身形比較健美\\n\\n\\n剛當媽媽了，過一陣子就要換石井優希也當媽了\\n\\n剛剛重新查一下，體重打錯了@@\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最後作extract\n",
    "main_content = html.find(\"div\", {\"id\": \"main-content\"})\n",
    "# print(main_content)\n",
    "metas = main_content.find_all(\"div\", {\"class\": \"article-metaline\"})\n",
    "for m in metas:\n",
    "    m.extract()\n",
    "metas = main_content.find_all(\"div\", {\"class\": \"article-metaline-right\"})\n",
    "for m in metas:\n",
    "    m.extract()\n",
    "metas = main_content.find_all(\"div\", {\"class\": \"push\"})\n",
    "for m in metas:\n",
    "    m.extract()\n",
    "metas = main_content.find_all(\"span\", {\"class\": \"f2\"})\n",
    "for m in metas:\n",
    "    m_text = m.get_text()\n",
    "    if \"※ 發信站:\" in m_text:\n",
    "        m.extract()\n",
    "    if \"※ 文章網址:\" in m_text:\n",
    "        m.extract()\n",
    "    if \"編輯:\" in m_text:\n",
    "        m.extract()\n",
    "main_content = main_content.get_text()\n",
    "main_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7910b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def get_page(url, dirname = \".\"):\n",
    "    #url = \"https://www.ptt.cc/bbs/Beauty/M.1765596118.A.4A6.html\"\n",
    "    resp = req.urlopen(url)\n",
    "    content = resp.read()\n",
    "    html = bs.BeautifulSoup(content)\n",
    "\n",
    "    uid_text = \"-\"\n",
    "    board_text = \"-\"\n",
    "    title_text = \"-\"\n",
    "    date_text = \"-\"\n",
    "    push = []\n",
    "\n",
    "    metas = html.find_all(\"span\", {\"class\": \"article-meta-value\"})\n",
    "    if len(metas) == 4:\n",
    "        uid, board, title, date = metas\n",
    "        uid_text = uid.get_text().strip()\n",
    "        board_text = board.get_text().strip()\n",
    "        title_text = title.get_text().strip()\n",
    "        date_text = date.get_text().strip()\n",
    "\n",
    "    data = {\n",
    "        \"uid\": uid_text,\n",
    "        \"board\": board_text,\n",
    "        \"title\": title_text,\n",
    "        \"datetime\": date_text,\n",
    "        \"content\": \"\",\n",
    "        \"pushes\": push}\n",
    "\n",
    "    push_meta = html.find_all(\"div\", {\"class\": \"push\"})\n",
    "    for p in push_meta:\n",
    "        p_dic = {}\n",
    "        p_tag, p_uid, p_text, p_ipdatetime = p.find_all(\"span\")\n",
    "        tag_text = p_tag.get_text().strip()\n",
    "        uid_text = p_uid.get_text().strip()\n",
    "        text_text = p_text.get_text().strip().replace(\": \", \"\", 1)\n",
    "        ipdatetime_text = p_ipdatetime.get_text().strip()\n",
    "        dic = {\"推\": 1, \"噓\": -1, \"→\": 0}\n",
    "        type_value = dic.get(tag_text, 0)\n",
    "\n",
    "        p_dic = {\n",
    "            \"tag\": type_value,\n",
    "            \"uid\": uid_text,\n",
    "            \"text\": text_text,\n",
    "            \"ipdatetime\": ipdatetime_text\n",
    "        }\n",
    "        push.append(p_dic)\n",
    "\n",
    "    # 最後作extract\n",
    "    main_content = html.find(\"div\", {\"id\": \"main-content\"})\n",
    "    # print(main_content)\n",
    "    metas = main_content.find_all(\"div\", {\"class\": \"article-metaline\"})\n",
    "    for m in metas:\n",
    "        m.extract()\n",
    "    metas = main_content.find_all(\"div\", {\"class\": \"article-metaline-right\"})\n",
    "    for m in metas:\n",
    "        m.extract()\n",
    "    metas = main_content.find_all(\"div\", {\"class\": \"push\"})\n",
    "    for m in metas:\n",
    "        m.extract()\n",
    "    metas = main_content.find_all(\"span\", {\"class\": \"f2\"})\n",
    "    for m in metas:\n",
    "        m_text = m.get_text()\n",
    "        if \"※ 發信站:\" in m_text:\n",
    "            m.extract()\n",
    "        if \"※ 文章網址:\" in m_text:\n",
    "            m.extract()\n",
    "        if \"編輯:\" in m_text:\n",
    "            m.extract()\n",
    "    data[\"content\"] = main_content.get_text()\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    f = open(f\"{dirname}/meta.json\", \"w\", encoding=\"utf-8\")\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580c9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "import os\n",
    "\n",
    "# .是這層  ..是上一層\n",
    "# url = \"https://i.imgur.com/opGiGRV.jpeg\"\n",
    "def get_pic(url, dirname = \".\"):\n",
    "    h = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    name = url.split(\"/\")[-1]\n",
    "\n",
    "    r = req.Request(url, headers=h)\n",
    "    resp = req.urlopen(r)\n",
    "    content = resp.read()\n",
    "    # 1.純文字        2.非純文字\n",
    "    # r/w + encoding    rb/wb\n",
    "    f = open(f\"{dirname}/{name}\", \"wb\")\n",
    "    f.write(content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c1e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.ptt.cc/bbs/Beauty/M.1765596118.A.4A6.html\"\n",
    "def get_all(url, dirname = \".\"):\n",
    "    resp = req.urlopen(url)\n",
    "    content = resp.read()\n",
    "    html = bs.BeautifulSoup(content)\n",
    "    link = html.find_all(\"a\")\n",
    "    filename = dirname + \"/\" + url.split(\"/\")[-1]\n",
    "\n",
    "    get_page(url, filename)\n",
    "    allow_subnames = [\"jpg\", \"jpeg\", \"png\", \"gif\", \"mp4\"]\n",
    "    for l in link:\n",
    "        href = l[\"href\"]\n",
    "        sub = href.split(\".\")[-1]\n",
    "        if sub.lower() in allow_subnames:\n",
    "            get_pic(href, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdeecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = 3\n",
    "for i in range(pn):\n",
    "    print(f\"正在印第{i}頁\")\n",
    "    urlT = f\"https://www.ptt.cc/bbs/Beauty/index{4001 - i}.html\"\n",
    "    resp = req.urlopen(urlT)\n",
    "    content = resp.read()\n",
    "    html = bs.BeautifulSoup(content)\n",
    "    link = html.find_all(\"div\", {\"class\": \"title\"})\n",
    "    for l in link:\n",
    "        a = l.find(\"a\")\n",
    "        if a is None:\n",
    "            continue\n",
    "        if(\"公告\" in a.get_text()):\n",
    "            continue\n",
    "        link = \"https://www.ptt.cc\" + a[\"href\"]\n",
    "        os.makedirs(\"PTT\", exist_ok=True)\n",
    "        get_all(link, \"PTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "405150e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "target = {\"name\": \"吳宗憲\", \"hight\" : 165}\n",
    "# list/dict->str\n",
    "s = json.dumps(target)\n",
    "f = open(\"a.json\", \"w\", encoding=\"utf-8\")\n",
    "f.write(s)\n",
    "f.close()\n",
    "\n",
    "#\n",
    "target = {\"name\": \"吳宗憲\", \"hight\" : 165}\n",
    "f = open(\"b.json\", \"w\", encoding=\"utf-8\")\n",
    "json.dump(target, f, ensure_ascii=False,indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8298c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"b.json\", encoding=\"utf-8\")\n",
    "content = f.read()\n",
    "print(json.loads(content))\n",
    "f.close()\n",
    "\n",
    "f = open(\"b.json\", encoding=\"utf-8\")\n",
    "print(json.load(f))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
